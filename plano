PLANO COMPLETO: REAL-TIME MONITORING + PREDICTIVE ANALYTICS + NLP ASSISTANT
Vou detalhar a implementação completa dos 3 recursos mais impactantes para operadores HWA/TWS.

📋 PLANO DE IMPLEMENTAÇÃO ESTRUTURADO
FASE 1: Real-time Job Monitoring (Semanas 1-2)
FASE 2: Predictive Analytics Engine (Semanas 3-4)
FASE 3: NLP Assistant (Semanas 5-6)
🔧 STACK TECNOLÓGICO COMPLETO
Backend Dependencies (pyproject.toml)
text
[project]
dependencies = [
    # Existing dependencies
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "httpx>=0.25.0",
    
    # WebSockets & Real-time
    "fastapi-websocket-pubsub>=0.3.0",
    "redis>=5.0.0",
    "celery[redis]>=5.3.0",
    
    # Machine Learning & Analytics
    "scikit-learn>=1.3.0",
    "pandas>=2.1.0",
    "numpy>=1.24.0",
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "joblib>=1.3.0",
    
    # NLP & AI
    "openai>=1.3.0",
    "langchain>=0.0.340",
    "tiktoken>=0.5.0",
    "sentence-transformers>=2.2.0",
    "faiss-cpu>=1.7.4",
    
    # Time Series & Forecasting
    "prophet>=1.1.0",
    "statsmodels>=0.14.0",
    
    # Database & Storage
    "sqlalchemy>=2.0.0",
    "alembic>=1.12.0",
    "psycopg2-binary>=2.9.0",  # PostgreSQL
    
    # Monitoring & Observability
    "prometheus-client>=0.18.0",
    "grafana-api>=1.0.3",
]

[project.optional-dependencies]
dev = [
    # Existing dev deps
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    
    # ML Development
    "jupyter>=1.0.0",
    "mlflow>=2.7.0",
    "tensorboard>=2.14.0",
]
Frontend Dependencies (package.json)
json
{
  "dependencies": {
    "socket.io-client": "^4.7.2",
    "chart.js": "^4.4.0",
    "chartjs-adapter-date-fns": "^3.0.0",
    "date-fns": "^2.30.0",
    "plotly.js-dist": "^2.26.0",
    "d3": "^7.8.5",
    "vis-network": "^9.1.6",
    "marked": "^9.1.2",
    "highlight.js": "^11.9.0"
  },
  "devDependencies": {
    "webpack": "^5.88.0",
    "babel-loader": "^9.1.3"
  }
}
🏗️ ARQUITETURA DO SISTEMA
Nova Estrutura de Diretórios
text
src/
├── core/
│   ├── config.py
│   └── database.py          # 🆕 Database models
├── services/
│   ├── monitoring/
│   │   ├── __init__.py
│   │   ├── websocket.py     # 🆕 WebSocket manager
│   │   ├── job_monitor.py   # 🆕 Job monitoring service
│   │   └── alert_engine.py  # 🆕 Alert management
│   ├── ml/
│   │   ├── __init__.py
│   │   ├── predictor.py     # 🆕 ML prediction engine
│   │   ├── models.py        # 🆕 ML model definitions
│   │   └── trainer.py       # 🆕 Model training pipeline
│   └── nlp/
│       ├── __init__.py
│       ├── assistant.py     # 🆕 NLP chatbot
│       ├── knowledge_base.py # 🆕 Knowledge management
│       └── embeddings.py    # 🆕 Vector embeddings
├── models/
│   ├── __init__.py
│   ├── database.py          # 🆕 SQLAlchemy models
│   └── schemas.py           # 🆕 Pydantic schemas
├── tasks/
│   ├── __init__.py
│   ├── monitoring.py        # 🆕 Celery background tasks
│   └── ml_training.py       # 🆕 ML training tasks
└── api/
    ├── websockets.py        # 🆕 WebSocket endpoints
    ├── monitoring.py        # 🆕 Monitoring API
    └── assistant.py         # 🆕 NLP Assistant API
🔥 FASE 1: REAL-TIME JOB MONITORING
1.1 WebSocket Infrastructure
python
# src/services/monitoring/websocket.py
import asyncio
import json
import logging
from typing import Dict, Set
from fastapi import WebSocket, WebSocketDisconnect
import redis.asyncio as redis

class WebSocketManager:
    def __init__(self):
        # Active connections by user/session
        self.active_connections: Dict[str, Set[WebSocket]] = {}
        self.redis_client: redis.Redis = None
        
    async def initialize(self):
        """Initialize Redis connection for pub/sub"""
        self.redis_client = await redis.from_url(
            "redis://localhost:6379", 
            decode_responses=True
        )
        
    async def connect(self, websocket: WebSocket, user_id: str):
        """Accept WebSocket connection and add to active connections"""
        await websocket.accept()
        if user_id not in self.active_connections:
            self.active_connections[user_id] = set()
        self.active_connections[user_id].add(websocket)
        logging.info(f"WebSocket connected for user: {user_id}")
        
    async def disconnect(self, websocket: WebSocket, user_id: str):
        """Remove WebSocket connection"""
        if user_id in self.active_connections:
            self.active_connections[user_id].discard(websocket)
            if not self.active_connections[user_id]:
                del self.active_connections[user_id]
        logging.info(f"WebSocket disconnected for user: {user_id}")
        
    async def send_personal_message(self, message: dict, user_id: str):
        """Send message to specific user's connections"""
        if user_id in self.active_connections:
            disconnected = []
            for websocket in self.active_connections[user_id]:
                try:
                    await websocket.send_json(message)
                except:
                    disconnected.append(websocket)
            
            # Clean up disconnected sockets
            for ws in disconnected:
                await self.disconnect(ws, user_id)
                
    async def broadcast(self, message: dict):
        """Broadcast message to all connected clients"""
        for user_id in list(self.active_connections.keys()):
            await self.send_personal_message(message, user_id)
            
    async def subscribe_to_updates(self):
        """Subscribe to Redis pub/sub for real-time updates"""
        pubsub = self.redis_client.pubsub()
        await pubsub.subscribe("job_updates", "alert_notifications")
        
        async for message in pubsub.listen():
            if message["type"] == "message":
                try:
                    data = json.loads(message["data"])
                    await self.broadcast(data)
                except Exception as e:
                    logging.error(f"Error processing pub/sub message: {e}")

# Global WebSocket manager instance
ws_manager = WebSocketManager()
1.2 Job Monitoring Service
python
# src/services/monitoring/job_monitor.py
import asyncio
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import redis.asyncio as redis
from src.hwa_connector import HWAClient
from src.models.database import JobStatusHistory, AlertRule
from src.services.monitoring.websocket import ws_manager

@dataclass
class JobStatusEvent:
    job_id: str
    job_name: str
    old_status: str
    new_status: str
    workstation: str
    timestamp: datetime
    duration: Optional[int] = None
    error_message: Optional[str] = None

class JobMonitoringService:
    def __init__(self):
        self.redis_client: redis.Redis = None
        self.monitoring_active = False
        self.poll_interval = 30  # seconds
        self.job_cache: Dict[str, dict] = {}
        
    async def initialize(self):
        """Initialize monitoring service"""
        self.redis_client = await redis.from_url(
            "redis://localhost:6379",
            decode_responses=True
        )
        
    async def start_monitoring(self):
        """Start continuous job monitoring"""
        self.monitoring_active = True
        logging.info("Job monitoring service started")
        
        while self.monitoring_active:
            try:
                await self._poll_job_status()
                await asyncio.sleep(self.poll_interval)
            except Exception as e:
                logging.error(f"Error in monitoring loop: {e}")
                await asyncio.sleep(5)  # Short delay on error
                
    async def stop_monitoring(self):
        """Stop monitoring service"""
        self.monitoring_active = False
        logging.info("Job monitoring service stopped")
        
    async def _poll_job_status(self):
        """Poll HWA for current job status"""
        async with HWAClient() as client:
            # Get current job streams
            current_jobs = await client.plan.query_job_streams()
            
            # Process each job for changes
            for job in current_jobs:
                await self._process_job_update(job)
                
            # Update cache
            self.job_cache = {job['jobStreamName']: job for job in current_jobs}
            
    async def _process_job_update(self, job_data: dict):
        """Process individual job for status changes"""
        job_name = job_data.get('jobStreamName')
        current_status = job_data.get('status', 'UNKNOWN')
        
        # Check if this is a status change
        if job_name in self.job_cache:
            old_job = self.job_cache[job_name]
            old_status = old_job.get('status', 'UNKNOWN')
            
            if old_status != current_status:
                # Status changed - create event
                event = JobStatusEvent(
                    job_id=job_data.get('id', job_name),
                    job_name=job_name,
                    old_status=old_status,
                    new_status=current_status,
                    workstation=job_data.get('workstationName', ''),
                    timestamp=datetime.now()
                )
                
                await self._handle_status_change(event)
        else:
            # New job discovered
            event = JobStatusEvent(
                job_id=job_data.get('id', job_name),
                job_name=job_name,
                old_status='NEW',
                new_status=current_status,
                workstation=job_data.get('workstationName', ''),
                timestamp=datetime.now()
            )
            
            await self._handle_status_change(event)
            
    async def _handle_status_change(self, event: JobStatusEvent):
        """Handle job status change event"""
        logging.info(f"Job status change: {event.job_name} {event.old_status} → {event.new_status}")
        
        # Store in database
        await self._store_status_history(event)
        
        # Check alert rules
        await self._check_alert_rules(event)
        
        # Publish real-time update
        await self._publish_realtime_update(event)
        
    async def _store_status_history(self, event: JobStatusEvent):
        """Store job status change in database"""
        # Implementation depends on your database setup
        # This would use SQLAlchemy to store the event
        pass
        
    async def _check_alert_rules(self, event: JobStatusEvent):
        """Check if status change triggers any alert rules"""
        # Critical status changes that need immediate attention
        critical_statuses = ['ABEND', 'ERROR', 'FAIL']
        
        if event.new_status in critical_statuses:
            alert_data = {
                "type": "job_failure",
                "severity": "HIGH",
                "job_name": event.job_name,
                "status": event.new_status,
                "workstation": event.workstation,
                "timestamp": event.timestamp.isoformat(),
                "message": f"Job {event.job_name} failed with status {event.new_status}"
            }
            
            await self._send_alert(alert_data)
            
    async def _send_alert(self, alert_data: dict):
        """Send alert notification"""
        # Publish to Redis for real-time notifications
        await self.redis_client.publish(
            "alert_notifications",
            json.dumps(alert_data)
        )
        
        # Could also send to external systems (Slack, email, etc.)
        logging.warning(f"ALERT: {alert_data['message']}")
        
    async def _publish_realtime_update(self, event: JobStatusEvent):
        """Publish real-time update via WebSocket"""
        update_data = {
            "type": "job_status_update",
            "data": {
                "job_id": event.job_id,
                "job_name": event.job_name,
                "old_status": event.old_status,
                "new_status": event.new_status,
                "workstation": event.workstation,
                "timestamp": event.timestamp.isoformat()
            }
        }
        
        await self.redis_client.publish(
            "job_updates",
            json.dumps(update_data)
        )

# Global monitoring service instance
job_monitor = JobMonitoringService()
1.3 WebSocket API Endpoints
python
# src/api/websockets.py
from fastapi import APIRouter, WebSocket, WebSocketDisconnect, Depends
from src.services.monitoring.websocket import ws_manager
import logging

router = APIRouter()

@router.websocket("/ws/monitoring/{user_id}")
async def websocket_monitoring_endpoint(websocket: WebSocket, user_id: str):
    """WebSocket endpoint for real-time monitoring updates"""
    await ws_manager.connect(websocket, user_id)
    
    try:
        while True:
            # Keep connection alive and handle any incoming messages
            data = await websocket.receive_text()
            
            # Handle client messages (e.g., subscription preferences)
            try:
                message = json.loads(data)
                if message.get("type") == "subscribe":
                    # Handle subscription to specific job types/workstations
                    pass
            except json.JSONDecodeError:
                pass
                
    except WebSocketDisconnect:
        await ws_manager.disconnect(websocket, user_id)
    except Exception as e:
        logging.error(f"WebSocket error for user {user_id}: {e}")
        await ws_manager.disconnect(websocket, user_id)
🧠 FASE 2: PREDICTIVE ANALYTICS ENGINE
2.1 Machine Learning Models
python
# src/services/ml/predictor.py
import joblib
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from prophet import Prophet
import logging

class JobFailurePredictorML:
    def __init__(self):
        self.failure_model: RandomForestClassifier = None
        self.anomaly_detector: IsolationForest = None
        self.scaler: StandardScaler = StandardScaler()
        self.feature_columns = [
            'avg_runtime', 'runtime_variance', 'failure_rate_7d',
            'workstation_load', 'time_of_day', 'day_of_week',
            'consecutive_failures', 'sla_breach_history'
        ]
        
    async def train_failure_prediction_model(self, historical_data: pd.DataFrame):
        """Train ML model to predict job failures"""
        logging.info("Training job failure prediction model...")
        
        # Feature engineering
        features_df = await self._engineer_features(historical_data)
        
        # Prepare training data
        X = features_df[self.feature_columns]
        y = features_df['failed']  # Binary: 0=success, 1=failure
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Train Random Forest model
        self.failure_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced'  # Handle imbalanced data
        )
        
        self.failure_model.fit(X_train_scaled, y_train)
        
        # Evaluate model
        y_pred = self.failure_model.predict(X_test_scaled)
        report = classification_report(y_test, y_pred)
        logging.info(f"Model performance:\n{report}")
        
        # Save model
        self._save_model()
        
        return {
            "accuracy": self.failure_model.score(X_test_scaled, y_test),
            "feature_importance": dict(zip(
                self.feature_columns,
                self.failure_model.feature_importances_
            ))
        }
        
    async def predict_job_failure(self, job_data: dict) -> dict:
        """Predict probability of job failure"""
        if not self.failure_model:
            await self._load_model()
            
        # Extract features for this job
        features = await self._extract_job_features(job_data)
        features_scaled = self.scaler.transform([features])
        
        # Get prediction
        failure_prob = self.failure_model.predict_proba(features_scaled)[0][1]
        prediction = self.failure_model.predict(features_scaled)[0]
        
        # Get feature contributions (SHAP-like explanation)
        feature_importance = dict(zip(
            self.feature_columns,
            self.failure_model.feature_importances_
        ))
        
        return {
            "job_name": job_data.get("jobStreamName"),
            "failure_probability": float(failure_prob),
            "prediction": "LIKELY_TO_FAIL" if prediction == 1 else "LIKELY_TO_SUCCEED",
            "confidence": max(failure_prob, 1 - failure_prob),
            "risk_factors": self._identify_risk_factors(features, feature_importance),
            "recommendation": self._get_recommendation(failure_prob)
        }
        
    async def detect_anomalies(self, recent_jobs: list) -> list:
        """Detect anomalous job behavior patterns"""
        if not self.anomaly_detector:
            await self._train_anomaly_detector()
            
        anomalies = []
        
        for job in recent_jobs:
            features = await self._extract_job_features(job)
            features_scaled = self.scaler.transform([features])
            
            # -1 for outliers, 1 for inliers
            is_anomaly = self.anomaly_detector.predict(features_scaled)[0] == -1
            
            if is_anomaly:
                anomaly_score = self.anomaly_detector.score_samples(features_scaled)[0]
                anomalies.append({
                    "job_name": job.get("jobStreamName"),
                    "anomaly_score": float(anomaly_score),
                    "anomaly_type": self._classify_anomaly_type(features),
                    "description": self._describe_anomaly(job, features)
                })
                
        return anomalies
        
    async def _engineer_features(self, historical_data: pd.DataFrame) -> pd.DataFrame:
        """Engineer features from raw historical data"""
        # Convert datetime columns
        historical_data['timestamp'] = pd.to_datetime(historical_data['timestamp'])
        
        # Time-based features
        historical_data['hour'] = historical_data['timestamp'].dt.hour
        historical_data['day_of_week'] = historical_data['timestamp'].dt.dayofweek
        historical_data['time_of_day'] = historical_data['hour'] / 24.0
        
        # Job performance features
        job_stats = historical_data.groupby('job_name').agg({
            'runtime_seconds': ['mean', 'std'],
            'failed': ['mean', 'sum']
        }).round(2)
        
        job_stats.columns = ['avg_runtime', 'runtime_variance', 'failure_rate_7d', 'total_failures']
        
        # Merge back to original data
        features_df = historical_data.merge(
            job_stats, 
            left_on='job_name', 
            right_index=True, 
            how='left'
        )
        
        # Fill missing values
        features_df = features_df.fillna(0)
        
        return features_df
        
    async def _extract_job_features(self, job_data: dict) -> list:
        """Extract features from current job data"""
        # This would extract real-time features based on job_data
        # For now, returning mock features
        return [
            job_data.get('avg_runtime', 300),      # avg runtime in seconds
            job_data.get('runtime_variance', 50),   # runtime variance
            job_data.get('failure_rate_7d', 0.1),  # 7-day failure rate
            job_data.get('workstation_load', 0.7), # workstation load
            datetime.now().hour / 24.0,            # time of day (0-1)
            datetime.now().weekday(),              # day of week (0-6)
            job_data.get('consecutive_failures', 0), # consecutive failures
            job_data.get('sla_breach_history', 0)   # SLA breach history
        ]
        
    def _identify_risk_factors(self, features: list, importance: dict) -> list:
        """Identify main risk factors for this prediction"""
        risk_factors = []
        
        for i, (feature_name, feature_value) in enumerate(zip(self.feature_columns, features)):
            if importance[feature_name] > 0.1:  # High importance threshold
                risk_factors.append({
                    "factor": feature_name,
                    "value": feature_value,
                    "importance": importance[feature_name],
                    "description": self._get_factor_description(feature_name, feature_value)
                })
                
        return sorted(risk_factors, key=lambda x: x['importance'], reverse=True)
        
    def _get_recommendation(self, failure_prob: float) -> str:
        """Get recommendation based on failure probability"""
        if failure_prob > 0.8:
            return "CRITICAL: Consider delaying or investigating this job before execution"
        elif failure_prob > 0.6:
            return "HIGH RISK: Monitor closely and have backup plan ready"
        elif failure_prob > 0.4:
            return "MEDIUM RISK: Review job dependencies and resource availability"
        else:
            return "LOW RISK: Job expected to complete successfully"
            
    def _save_model(self):
        """Save trained models to disk"""
        joblib.dump(self.failure_model, 'models/failure_predictor.joblib')
        joblib.dump(self.scaler, 'models/feature_scaler.joblib')
        
    async def _load_model(self):
        """Load trained models from disk"""
        try:
            self.failure_model = joblib.load('models/failure_predictor.joblib')
            self.scaler = joblib.load('models/feature_scaler.joblib')
        except FileNotFoundError:
            logging.warning("No trained model found. Please train the model first.")

# Global predictor instance
job_predictor = JobFailurePredictorML()
2.2 Workload Forecasting
python
# src/services/ml/forecasting.py
from prophet import Prophet
import pandas as pd
from datetime import datetime, timedelta
import logging

class WorkloadForecaster:
    def __init__(self):
        self.models = {}  # Store models by workstation/job type
        
    async def train_workload_forecast(self, historical_data: pd.DataFrame):
        """Train Prophet models for workload forecasting"""
        logging.info("Training workload forecasting models...")
        
        # Group by workstation for individual forecasting
        for workstation in historical_data['workstation'].unique():
            ws_data = historical_data[historical_data['workstation'] == workstation]
            
            # Prepare data for Prophet (needs 'ds' and 'y' columns)
            prophet_data = ws_data.groupby('date').agg({
                'job_count': 'sum',
                'total_runtime': 'sum',
                'cpu_usage': 'mean'
            }).reset_index()
            
            # Train separate models for different metrics
            for metric in ['job_count', 'total_runtime', 'cpu_usage']:
                model_data = prophet_data[['date', metric]].rename(
                    columns={'date': 'ds', metric: 'y'}
                )
                
                model = Prophet(
                    yearly_seasonality=True,
                    weekly_seasonality=True,
                    daily_seasonality=True,
                    changepoint_prior_scale=0.05
                )
                
                model.fit(model_data)
                self.models[f"{workstation}_{metric}"] = model
                
    async def forecast_workload(self, workstation: str, days_ahead: int = 7) -> dict:
        """Generate workload forecast for specific workstation"""
        forecasts = {}
        
        for metric in ['job_count', 'total_runtime', 'cpu_usage']:
            model_key = f"{workstation}_{metric}"
            
            if model_key in self.models:
                model = self.models[model_key]
                
                # Create future dataframe
                future = model.make_future_dataframe(periods=days_ahead)
                forecast = model.predict(future)
                
                # Extract forecast for requested period
                future_forecast = forecast.tail(days_ahead)
                
                forecasts[metric] = {
                    "predictions": future_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].to_dict('records'),
                    "trend": "increasing" if forecast['trend'].iloc[-1] > forecast['trend'].iloc[-7] else "decreasing",
                    "seasonality_strength": float(forecast[['weekly', 'yearly']].abs().mean().mean())
                }
                
        return {
            "workstation": workstation,
            "forecast_period": f"{days_ahead} days",
            "generated_at": datetime.now().isoformat(),
            "forecasts": forecasts
        }

# Global forecaster instance  
workload_forecaster = WorkloadForecaster()
🤖 FASE 3: NLP ASSISTANT
3.1 Knowledge Base & Embeddings
python
# src/services/nlp/knowledge_base.py
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict
import logging

class HWAKnowledgeBase:
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.knowledge_index = None
        self.knowledge_docs = []
        self.doc_embeddings = None
        
    async def initialize(self):
        """Initialize knowledge base with HWA/TWS documentation"""
        # Load HWA/TWS knowledge base
        await self._load_knowledge_base()
        await self._build_embeddings_index()
        
    async def _load_knowledge_base(self):
        """Load HWA/TWS knowledge documents"""
        # This would load from your documentation/knowledge base
        self.knowledge_docs = [
            {
                "id": "job_status_abend",
                "title": "Job Status: ABEND",
                "content": "ABEND status indicates abnormal termination of a job. Common causes: resource constraints, dependency failures, script errors. Resolution: Check job log, verify dependencies, restart if necessary.",
                "category": "troubleshooting",
                "tags": ["abend", "failure", "error"]
            },
            {
                "id": "job_dependencies",
                "title": "Job Dependencies Management",
                "content": "Job dependencies define execution order. Use predecessor/successor relationships. Critical path analysis shows longest sequence. Circular dependencies cause deadlocks.",
                "category": "scheduling",
                "tags": ["dependencies", "scheduling", "critical path"]
            },
            {
                "id": "workstation_status",
                "title": "Workstation Status Monitoring",
                "content": "Workstation statuses: LINK (online), UNLINK (offline), LIMITED (partial capacity). Monitor resource usage, connection health, job queue length.",
                "category": "monitoring",
                "tags": ["workstation", "status", "monitoring"]
            },
            {
                "id": "sla_management",
                "title": "SLA Management",
                "content": "SLA defines service level agreements for job completion times. Monitor SLA compliance, set up alerts for breaches, analyze trends for capacity planning.",
                "category": "sla",
                "tags": ["sla", "compliance", "monitoring"]
            }
            # Add more knowledge base entries...
        ]
        
    async def _build_embeddings_index(self):
        """Build FAISS index for semantic search"""
        logging.info("Building knowledge base embeddings index...")
        
        # Create embeddings for all documents
        doc_texts = [doc["content"] for doc in self.knowledge_docs]
        self.doc_embeddings = self.embedding_model.encode(doc_texts)
        
        # Build FAISS index
        dimension = self.doc_embeddings.shape[1]
        self.knowledge_index = faiss.IndexFlatIP(dimension)  # Inner product for similarity
        
        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(self.doc_embeddings)
        self.knowledge_index.add(self.doc_embeddings)
        
        logging.info(f"Knowledge base index built with {len(self.knowledge_docs)} documents")
        
    async def search_knowledge(self, query: str, top_k: int = 3) -> List[Dict]:
        """Search knowledge base using semantic similarity"""
        # Encode query
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # Search index
        scores, indices = self.knowledge_index.search(query_embedding, top_k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx != -1:  # Valid result
                doc = self.knowledge_docs[idx]
                results.append({
                    "document": doc,
                    "relevance_score": float(score),
                    "rank": i + 1
                })
                
        return results
        
    async def get_context_for_query(self, query: str, max_context_length: int = 2000) -> str:
        """Get relevant context for answering a query"""
        search_results = await self.search_knowledge(query, top_k=5)
        
        context_parts = []
        current_length = 0
        
        for result in search_results:
            doc = result["document"]
            content = f"Title: {doc['title']}\nContent: {doc['content']}\n\n"
            
            if current_length + len(content) <= max_context_length:
                context_parts.append(content)
                current_length += len(content)
            else:
                break
                
        return "".join(context_parts)

# Global knowledge base instance
hwa_kb = HWAKnowledgeBase()
3.2 NLP Assistant with OpenAI
python
# src/services/nlp/assistant.py
import openai
import json
import logging
from typing import Dict, List, Optional
from datetime import datetime
from src.services.nlp.knowledge_base import hwa_kb
from src.hwa_connector import HWAClient

class HWAAssistant:
    def __init__(self):
        self.conversation_history = {}
        self.system_prompt = """
        You are an expert HCL Workload Automation (HWA/TWS) operations assistant. 
        You help operators monitor jobs, troubleshoot issues, and optimize workloads.
        
        Your capabilities:
        - Analyze job statuses and performance
        - Provide troubleshooting guidance
        - Explain HWA/TWS concepts
        - Suggest optimization strategies
        - Help with operational procedures
        
        Always provide practical, actionable advice based on current system state.
        Use the provided context from the knowledge base and current system data.
        """
        
    async def chat(self, user_id: str, message: str) -> Dict:
        """Process user message and generate response"""
        logging.info(f"Processing message from user {user_id}: {message}")
        
        # Get conversation history
        if user_id not in self.conversation_history:
            self.conversation_history[user_id] = []
            
        # Add user message to history
        self.conversation_history[user_id].append({
            "role": "user",
            "content": message,
            "timestamp": datetime.now().isoformat()
        })
        
        # Analyze message intent
        intent = await self._analyze_intent(message)
        
        # Get relevant context
        context = await self._gather_context(message, intent)
        
        # Generate response
        response = await self._generate_response(user_id, message, context, intent)
        
        # Add assistant response to history
        self.conversation_history[user_id].append({
            "role": "assistant", 
            "content": response["text"],
            "timestamp": datetime.now().isoformat()
        })
        
        return response
        
    async def _analyze_intent(self, message: str) -> Dict:
        """Analyze user message intent"""
        # Simple intent classification (could use ML model)
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["status", "job", "running", "failed"]):
            return {"category": "job_status", "action": "query"}
        elif any(word in message_lower for word in ["error", "abend", "failed", "problem"]):
            return {"category": "troubleshooting", "action": "diagnose"}
        elif any(word in message_lower for word in ["help", "how", "what", "explain"]):
            return {"category": "information", "action": "explain"}
        elif any(word in message_lower for word in ["cancel", "hold", "release", "rerun"]):
            return {"category": "job_action", "action": "execute"}
        else:
            return {"category": "general", "action": "assist"}
            
    async def _gather_context(self, message: str, intent: Dict) -> Dict:
        """Gather relevant context for response generation"""
        context = {
            "knowledge_base": "",
            "current_system_state": {},
            "relevant_jobs": []
        }
        
        # Get knowledge base context
        kb_context = await hwa_kb.get_context_for_query(message)
        context["knowledge_base"] = kb_context
        
        # Get current system state if relevant
        if intent["category"] in ["job_status", "troubleshooting"]:
            try:
                async with HWAClient() as client:
                    # Get current jobs
                    jobs = await client.plan.query_job_streams()
                    workstations = await client.model.query_workstations()
                    
                    context["current_system_state"] = {
                        "total_jobs": len(jobs),
                        "running_jobs": len([j for j in jobs if j.get('status') == 'EXEC']),
                        "failed_jobs": len([j for j in jobs if j.get('status') == 'ABEND']),
                        "workstations": len(workstations),
                        "online_workstations": len([w for w in workstations if w.get('status') == 'LINK'])
                    }
                    
                    # Find relevant jobs mentioned in message
                    context["relevant_jobs"] = [
                        job for job in jobs 
                        if any(keyword in job.get('jobStreamName', '').lower() 
                              for keyword in message.lower().split())
                    ]
                    
            except Exception as e:
                logging.error(f"Error getting system context: {e}")
                
        return context
        
    async def _generate_response(self, user_id: str, message: str, context: Dict, intent: Dict) -> Dict:
        """Generate response using OpenAI"""
        try:
            # Build conversation for OpenAI
            messages = [
                {"role": "system", "content": self.system_prompt}
            ]
            
            # Add context if available
            if context["knowledge_base"]:
                context_message = f"Relevant HWA/TWS Knowledge:\n{context['knowledge_base']}"
                messages.append({"role": "system", "content": context_message})
                
            if context["current_system_state"]:
                state_message = f"Current System State:\n{json.dumps(context['current_system_state'], indent=2)}"
                messages.append({"role": "system", "content": state_message})
                
            # Add recent conversation history
            recent_history = self.conversation_history[user_id][-6:]  # Last 3 exchanges
            messages.extend([
                {"role": msg["role"], "content": msg["content"]} 
                for msg in recent_history[:-1]  # Exclude current message
            ])
            
            # Add current message
            messages.append({"role": "user", "content": message})
            
            # Generate response
            response = await openai.ChatCompletion.acreate(
                model="gpt-4",
                messages=messages,
                temperature=0.7,
                max_tokens=500,
                presence_penalty=0.1
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # Check if response suggests actions
            suggested_actions = await self._extract_suggested_actions(response_text, intent)
            
            return {
                "text": response_text,
                "intent": intent,
                "suggested_actions": suggested_actions,
                "context_used": bool(context["knowledge_base"] or context["current_system_state"]),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logging.error(f"Error generating response: {e}")
            return {
                "text": "I'm sorry, I'm having trouble processing your request right now. Please try again.",
                "intent": intent,
                "suggested_actions": [],
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            
    async def _extract_suggested_actions(self, response_text: str, intent: Dict) -> List[Dict]:
        """Extract actionable items from response"""
        actions = []
        
        if intent["category"] == "job_action":
            # Parse potential job actions from response
            response_lower = response_text.lower()
            
            if "cancel" in response_lower:
                actions.append({"type": "job_action", "action": "cancel", "description": "Cancel job"})
            if "hold" in response_lower:
                actions.append({"type": "job_action", "action": "hold", "description": "Hold job"})
            if "release" in response_lower:
                actions.append({"type": "job_action", "action": "release", "description": "Release job"})
            if "rerun" in response_lower:
                actions.append({"type": "job_action", "action": "rerun", "description": "Rerun job"})
                
        elif intent["category"] == "troubleshooting":
            actions.append({"type": "view_logs", "description": "View job logs for detailed error information"})
            actions.append({"type": "check_dependencies", "description": "Check job dependencies"})
            
        return actions
        
    async def get_conversation_summary(self, user_id: str) -> Dict:
        """Get summary of conversation for user"""
        if user_id not in self.conversation_history:
            return {"message": "No conversation history found"}
            
        history = self.conversation_history[user_id]
        
        return {
            "total_messages": len(history),
            "conversation_start": history[0]["timestamp"] if history else None,
            "last_interaction": history[-1]["timestamp"] if history else None,
            "recent_topics": await self._extract_topics(history[-10:])  # Last 10 messages
        }
        
    async def _extract_topics(self, messages: List[Dict]) -> List[str]:
        """Extract main topics from conversation"""
        # Simple keyword extraction (could use more sophisticated NLP)
        topics = set()
        
        for msg in messages:
            content = msg["content"].lower()
            
            if any(word in content for word in ["job", "status", "running", "failed"]):
                topics.add("job_monitoring")
            if any(word in content for word in ["error", "abend", "problem", "troubleshoot"]):
                topics.add("troubleshooting")
            if any(word in content for word in ["workstation", "server", "resource"]):
                topics.add("resource_management")
            if any(word in content for word in ["schedule", "dependency", "plan"]):
                topics.add("scheduling")
                
        return list(topics)

# Global assistant instance
hwa_assistant = HWAAssistant()
3.3 Assistant API Endpoints
python
# src/api/assistant.py
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from src.services.nlp.assistant import hwa_assistant
from src.services.nlp.knowledge_base import hwa_kb

router = APIRouter()

class ChatMessage(BaseModel):
    message: str
    user_id: str

class ChatResponse(BaseModel):
    response: str
    suggested_actions: list
    intent: dict
    timestamp: str

@router.post("/chat", response_model=ChatResponse)
async def chat_with_assistant(chat_msg: ChatMessage):
    """Chat with HWA assistant"""
    response = await hwa_assistant.chat(chat_msg.user_id, chat_msg.message)
    
    return ChatResponse(
        response=response["text"],
        suggested_actions=response["suggested_actions"],
        intent=response["intent"],
        timestamp=response["timestamp"]
    )

@router.get("/conversation/{user_id}")
async def get_conversation_history(user_id: str):
    """Get conversation history for user"""
    return await hwa_assistant.get_conversation_summary(user_id)

@router.post("/search-knowledge")
async def search_knowledge_base(query: str, top_k: int = 5):
    """Search HWA knowledge base"""
    results = await hwa_kb.search_knowledge(query, top_k)
    return {"query": query, "results": results}
🚀 IMPLEMENTAÇÃO FRONTEND
Frontend WebSocket Integration
javascript
// static/js/realtime-monitoring.js
class RealtimeMonitoring {
    constructor() {
        this.socket = null;
        this.userId = this.generateUserId();
        this.reconnectAttempts = 0;
        this.maxReconnectAttempts = 5;
    }
    
    connect() {
        const wsUrl = `ws://localhost:63136/ws/monitoring/${this.userId}`;
        this.socket = new WebSocket(wsUrl);
        
        this.socket.onopen = () => {
            console.log('WebSocket connected');
            this.reconnectAttempts = 0;
            this.showConnectionStatus('connected');
        };
        
        this.socket.onmessage = (event) => {
            const data = JSON.parse(event.data);
            this.handleRealtimeUpdate(data);
        };
        
        this.socket.onclose = () => {
            console.log('WebSocket disconnected');
            this.showConnectionStatus('disconnected');
            this.attemptReconnect();
        };
        
        this.socket.onerror = (error) => {
            console.error('WebSocket error:', error);
            this.showConnectionStatus('error');
        };
    }
    
    handleRealtimeUpdate(data) {
        switch(data.type) {
            case 'job_status_update':
                this.updateJobStatus(data.data);
                break;
            case 'alert_notification':
                this.showAlert(data.data);
                break;
            case 'prediction_update':
                this.updatePrediction(data.data);
                break;
        }
    }
    
    updateJobStatus(jobData) {
        // Update job status in real-time
        const jobCard = document.querySelector(`[data-job-id="${jobData.job_id}"]`);
        if (jobCard) {
            const statusElement = jobCard.querySelector('.job-status');
            statusElement.textContent = jobData.new_status;
            statusElement.className = `job-status status-${jobData.new_status.toLowerCase()}`;
            
            // Add animation for status change
            jobCard.classList.add('status-changed');
            setTimeout(() => jobCard.classList.remove('status-changed'), 2000);
        }
    }
    
    showAlert(alertData) {
        // Show real-time alert
        const alertHtml = `
            <div class="alert alert-${alertData.severity.toLowerCase()}" role="alert">
                <strong>${alertData.type.toUpperCase()}:</strong> ${alertData.message}
                <button type="button" class="close" onclick="this.parentElement.remove()">
                    <span>&times;</span>
                </button>
            </div>
        `;
        
        document.getElementById('alerts-container').insertAdjacentHTML('afterbegin', alertHtml);
    }
}

// Initialize real-time monitoring
const realtimeMonitoring = new RealtimeMonitoring();
document.addEventListener('DOMContentLoaded', () => {
    realtimeMonitoring.connect();
});
📈 CRONOGRAMA DETALHADO (6 SEMANAS)
Semana	Fase	Entregáveis	Recursos
1	WebSocket Setup	WebSocket manager, basic monitoring	1 dev backend
2	Job Monitoring	Real-time job updates, alerting	1 dev backend + 1 frontend
3	ML Foundation	Data pipeline, feature engineering	1 dev ML/backend
4	Prediction Models	Failure prediction, anomaly detection	1 dev ML
5	NLP Knowledge Base	Knowledge base, semantic search	1 dev NLP/backend
6	NLP Assistant	Chatbot, integration, testing	1 dev full-stack
🎯 PRÓXIMOS PASSOS IMEDIATOS
Install Dependencies: Adicionar packages no pyproject.toml

Database Setup: Criar tabelas para histórico e alertas

Redis Setup: Configurar Redis para WebSocket pub/sub

WebSocket Implementation: Implementar WebSocketManager

Testing: Criar testes unitários para cada componente
